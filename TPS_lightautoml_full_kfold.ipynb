{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T17:24:13.038257Z","iopub.status.busy":"2024-07-18T17:24:13.037557Z","iopub.status.idle":"2024-07-18T17:24:49.042400Z","shell.execute_reply":"2024-07-18T17:24:49.041283Z","shell.execute_reply.started":"2024-07-18T17:24:13.038204Z"},"trusted":true},"outputs":[],"source":["# Essential DS libraries\n","import numpy as np\n","import pandas as pd\n","import gc\n","from sklearn.metrics import roc_auc_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","# LightAutoML presets, task and report generation\n","from lightautoml.automl.presets.tabular_presets import TabularAutoML\n","from lightautoml.tasks import Task\n","\n","pd.set_option('display.max_columns', 50)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-18T16:46:17.568031Z","iopub.status.busy":"2024-07-18T16:46:17.567603Z","iopub.status.idle":"2024-07-18T16:46:54.971281Z","shell.execute_reply":"2024-07-18T16:46:54.970299Z","shell.execute_reply.started":"2024-07-18T16:46:17.567998Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>brand</th>\n","      <th>model</th>\n","      <th>model_year</th>\n","      <th>milage</th>\n","      <th>fuel_type</th>\n","      <th>engine</th>\n","      <th>transmission</th>\n","      <th>ext_col</th>\n","      <th>int_col</th>\n","      <th>accident</th>\n","      <th>clean_title</th>\n","      <th>price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>MINI</td>\n","      <td>Cooper S Base</td>\n","      <td>2007</td>\n","      <td>213000</td>\n","      <td>Gasoline</td>\n","      <td>172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel</td>\n","      <td>A/T</td>\n","      <td>Yellow</td>\n","      <td>Gray</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>4200</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Lincoln</td>\n","      <td>LS V8</td>\n","      <td>2002</td>\n","      <td>143250</td>\n","      <td>Gasoline</td>\n","      <td>252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel</td>\n","      <td>A/T</td>\n","      <td>Silver</td>\n","      <td>Beige</td>\n","      <td>At least 1 accident or damage reported</td>\n","      <td>Yes</td>\n","      <td>4999</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Chevrolet</td>\n","      <td>Silverado 2500 LT</td>\n","      <td>2002</td>\n","      <td>136731</td>\n","      <td>E85 Flex Fuel</td>\n","      <td>320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...</td>\n","      <td>A/T</td>\n","      <td>Blue</td>\n","      <td>Gray</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>13900</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Genesis</td>\n","      <td>G90 5.0 Ultimate</td>\n","      <td>2017</td>\n","      <td>19500</td>\n","      <td>Gasoline</td>\n","      <td>420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel</td>\n","      <td>Transmission w/Dual Shift Mode</td>\n","      <td>Black</td>\n","      <td>Black</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>45000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Mercedes-Benz</td>\n","      <td>Metris Base</td>\n","      <td>2021</td>\n","      <td>7388</td>\n","      <td>Gasoline</td>\n","      <td>208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n","      <td>7-Speed A/T</td>\n","      <td>Black</td>\n","      <td>Beige</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>97500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id          brand              model  model_year  milage      fuel_type  \\\n","0   0           MINI      Cooper S Base        2007  213000       Gasoline   \n","1   1        Lincoln              LS V8        2002  143250       Gasoline   \n","2   2      Chevrolet  Silverado 2500 LT        2002  136731  E85 Flex Fuel   \n","3   3        Genesis   G90 5.0 Ultimate        2017   19500       Gasoline   \n","4   4  Mercedes-Benz        Metris Base        2021    7388       Gasoline   \n","\n","                                              engine  \\\n","0       172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel   \n","1       252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel   \n","2  320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...   \n","3       420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel   \n","4       208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n","\n","                     transmission ext_col int_col  \\\n","0                             A/T  Yellow    Gray   \n","1                             A/T  Silver   Beige   \n","2                             A/T    Blue    Gray   \n","3  Transmission w/Dual Shift Mode   Black   Black   \n","4                     7-Speed A/T   Black   Beige   \n","\n","                                 accident clean_title  price  \n","0                           None reported         Yes   4200  \n","1  At least 1 accident or damage reported         Yes   4999  \n","2                           None reported         Yes  13900  \n","3                           None reported         Yes  45000  \n","4                           None reported         Yes  97500  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('data/train.csv')\n","test = pd.read_csv('data/test.csv')\n","\n","train_add = pd.read_csv('data/train_add.csv')\n","test_add = pd.read_csv('data/test_add.csv')\n","\n","train.head()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["all_data = pd.concat([train, train_add, test, test_add]).reset_index(drop = True)\n","all_data.head()\n","\n","def convert_engine(val):\n","    d = {\n","        'engine_HP': np.nan,\n","        'engine_L': np.nan,\n","        'engine_cylinder': np.nan,\n","        'engine_cylinder_type': np.nan,\n","        'engine_fuel_type': np.nan,\n","        'engine_type': np.nan,\n","        'engine_v': np.nan,\n","        'engine_vtype': np.nan\n","    }\n","    spl = val.replace('-', '').split(' ')\n","    pos_v1 = -1\n","    pos_v2 = -1\n","    for i, v in enumerate(spl):\n","        if v.endswith('HP'):\n","            d['engine_HP'] = float(v[:-2])\n","        elif v.endswith('L'):\n","            d['engine_L'] = float(v[:-1])\n","        elif v == 'Engine' and spl[i-1] == 'Cylinder':\n","            d['engine_cylinder'] = abs(float(spl[i-2].replace('V', '')))\n","        elif v == 'Fuel':\n","            d['engine_fuel_type'] = spl[i - 1]\n","        ##############\n","        elif v == 'Liter':\n","            d['engine_L'] = float(spl[i-1])\n","        elif v.startswith('V') or v.startswith('H') or v.startswith('I'):\n","            try:\n","                d['engine_cylinder'] = float(v[1:])\n","                d['engine_cylinder_type'] = v[0]\n","                pos_v1 = i\n","            except:\n","                pass\n","        elif v == 'DOHC' or v == 'OHV':\n","            d['engine_type'] = ' '.join(spl[i:])\n","            if pos_v2 != -1:\n","                d['engine_vtype'] = ' '.join(spl[pos_v2+1:i])\n","            elif pos_v1 != -1:\n","                d['engine_vtype'] = ' '.join(spl[pos_v1+1:i])\n","        elif v.endswith('V'):\n","            try:\n","                d['engine_v'] = float(v[:-1])\n","                pos_v2 = i\n","            except:\n","                pass\n","    return d\n","\n","all_data = pd.concat([all_data, pd.DataFrame.from_records(all_data['engine'].map(convert_engine).values)], axis = 1)\n","def milage_signs(val):\n","    v = str(val)\n","    for i in range(len(v) - 1, -1, -1):\n","        if v[i] != '0':\n","            break\n","    return (len(v) - 1 - i) / len(v) * 100\n","\n","\n","all_data['milage_signif_signs_perc'] = all_data['milage'].map(milage_signs)\n","for col in ['transmission', 'ext_col', 'int_col']:\n","    all_data[col] = all_data[col].str.lower()\n","\n","def convert_transmission(val):\n","    d = {\n","        'transmission_speeds_cnt': np.nan,\n","        'transmission_type': np.nan\n","    }\n","    spl = val.replace('/', '').split(' ')\n","    for i, v in enumerate(spl):\n","        if 'speed' in v:\n","            tmp = v.split('-')\n","            if len(tmp) > 1:\n","                if tmp[0] != 'single':\n","                    d['transmission_speeds_cnt'] = float(tmp[0])\n","                else:\n","                    d['transmission_speeds_cnt'] = 1.0\n","            else:\n","                d['transmission_speeds_cnt'] = float(spl[i-1])\n","        elif 'manual' in v or 'mt' in v:\n","            d['transmission_type'] = 'manual'\n","        elif 'automatic' in v or 'at' in v:\n","            d['transmission_type'] = 'automatic'\n","    return d\n","\n","all_data = pd.concat([all_data, pd.DataFrame.from_records(all_data['transmission'].map(convert_transmission).values)], axis = 1)\n","train2, train2_add, test2, test2_add = all_data.iloc[:len(train)], all_data.iloc[len(train):len(train)+len(train_add)], all_data.iloc[len(train)+len(train_add):len(train)+len(train_add)+len(test)], all_data.iloc[len(train)+len(train_add)+len(test):]\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","from lightautoml.text.nn_model import TorchUniversalModel\n","from lightautoml.text.embed import PLREmbedding, WeightedCatEmbedding, pooling_by_name\n","from lightautoml.ml_algo.torch_based.fttransformer.fttransformer_utils import Transformer\n","from collections import OrderedDict\n","from typing import List, Tuple, Type\n","from typing import Optional\n","from typing import Union\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","from lightautoml.text.nn_model import TorchUniversalModel\n","from lightautoml.text.embed import PLREmbedding, WeightedCatEmbedding, pooling_by_name\n","from lightautoml.ml_algo.torch_based.fttransformer.fttransformer_utils import Transformer\n","from collections import OrderedDict\n","from typing import List, Tuple, Type\n","from typing import Optional\n","from typing import Union\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","from transformers import BertConfig, BertModel, BertForSequenceClassification\n","from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n","\n","class BertEmbeddingsEmpty(nn.Module):\n","    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","\n","\n","    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, **kwargs):\n","        if input_ids is not None:\n","            input_shape = input_ids.size()\n","        else:\n","            input_shape = inputs_embeds.size()[:-1]\n","\n","        seq_length = input_shape[1]\n","\n","        embeddings = inputs_embeds\n","        return embeddings\n","class FTTransformer2(nn.Module):\n","    \"\"\"FT Transformer (https://arxiv.org/abs/2106.11959v2) from https://github.com/lucidrains/tab-transformer-pytorch/tree/main.\n","\n","    Args:\n","            pooling: Pooling used for the last step.\n","            n_out: Output dimension, 1 for binary prediction.\n","            embedding_size: Embeddings size.\n","            depth: Number of Attention Blocks inside Transformer.\n","            heads: Number of heads in Attention.\n","            attn_dropout: Post-Attention dropout.\n","            ff_dropout: Feed-Forward Dropout.\n","            dim_head: Attention head dimension.\n","            num_enc_layers: Number of Transformer layers.\n","            device: Device to compute on.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        *,\n","        pooling: str = \"mean\",\n","        n_out: int = 1,\n","        embedding_size: int = 32,\n","        depth: int = 4,\n","        heads: int = 1,\n","        attn_dropout: float = 0.1,\n","        ff_dropout: float = 0.1,\n","        dim_head: int = 32,\n","        num_enc_layers: int = 2,\n","        device: Union[str, torch.device] = \"cuda:0\",\n","        **kwargs,\n","    ):\n","        super(FTTransformer2, self).__init__()\n","        self.device = device\n","        self.pooling = pooling_by_name[pooling]()\n","        print('pooling', pooling)\n","    \n","        # transformer\n","        # self.transformer = nn.Sequential(\n","        #     *nn.ModuleList(\n","        #         [\n","        #             Transformer(\n","        #                 dim=embedding_size,\n","        #                 depth=depth,\n","        #                 heads=heads,\n","        #                 dim_head=dim_head,\n","        #                 attn_dropout=attn_dropout,\n","        #                 ff_dropout=ff_dropout,\n","        #             )\n","        #             for _ in range(num_enc_layers)\n","        #         ]\n","        #     )\n","        # )\n","        self.conf = {\"hidden_size\": 128, \"hidden_act\": \"gelu\", \"initializer_range\": 0.02, \"vocab_size\": 10, \"hidden_dropout_prob\": 0.1, \"num_attention_heads\": 2,\n","         \"type_vocab_size\": 2, \"max_position_embeddings\": 512, \"num_hidden_layers\": 2, \"intermediate_size\": 512, \"attention_probs_dropout_prob\": 0.1} #bert_tiny\n","        # self.conf = {\"hidden_size\": 512, \"hidden_act\": \"gelu\", \"initializer_range\": 0.02, \"vocab_size\": 10, \"hidden_dropout_prob\": 0.1, \"num_attention_heads\": 8,\n","        # \"type_vocab_size\": 2, \"max_position_embeddings\": 512, \"num_hidden_layers\": 4, \"intermediate_size\": 2048, \"attention_probs_dropout_prob\": 0.1} #bert_small\n","\n","        #self.transformer = BertModel(BertConfig(**self.conf))\n","        self.transformer = BertModel.from_pretrained(\"prajjwal1/bert-tiny\", output_attentions=True, output_hidden_states=True, cache_dir=\"./\")\n","        self.transformer.embeddings = BertEmbeddingsEmpty()\n","\n","        #peft_config = LoraConfig(task_type=TaskType.FEATURE_EXTRACTION, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n","\n","        #self.transformer = get_peft_model(self.transformer, peft_config)\n","        #print(self.transformer.print_trainable_parameters())\n","\n","        #self.fixup_initialization()\n","        # to logits\n","    \n","        self.to_logits = nn.Sequential(nn.BatchNorm1d(embedding_size), nn.Linear(embedding_size, n_out))\n","\n","        self.cls_token = nn.Embedding(2, embedding_size)\n","\n","    def fixup_initialization(self):\n","        temp_state_dic = {}\n","        en_layers = self.conf['num_hidden_layers']\n","\n","        for name, param in self.named_parameters():\n","            if 'weight' in name and param.data.dim() == 2:\n","                param = (9 * en_layers) ** (- 1 / 4) * param\n","                temp_state_dic[name] = param\n","            if np.any([1 if i in name else 0 for i in [\"linear1.weight\",\n","                        \"linear2.weight\",\n","                        \"self_attn.out_proj.weight\", #'self_attn.in_proj_weight', \n","                        ]]):\n","                temp_state_dic[name] = (0.67 * (en_layers) ** (- 1. / 4.)) * param\n","            elif name in [\"self_attn.v_proj.weight\",]:\n","                temp_state_dic[name] = (0.67 * (en_layers) ** (- 1. / 4.)) * (param * (2**0.5))\n","\n","        for name in self.state_dict():\n","            if name not in temp_state_dic:\n","                temp_state_dic[name] = self.state_dict()[name]\n","        self.load_state_dict(temp_state_dic)\n","\n","    def forward(self, embedded):\n","        \"\"\"Transform the input tensor.\n","\n","        Args:\n","            embedded : torch.Tensor\n","                embedded fields\n","\n","        Returns:\n","            torch.Tensor\n","\n","        \"\"\"\n","        cls_token = torch.unsqueeze(\n","            self.cls_token(torch.ones(embedded.shape[0], dtype=torch.int).to(self.device)), dim=1\n","        )\n","        x = torch.cat((cls_token, embedded), dim=1)\n","\n","        x = self.transformer(inputs_embeds=x, \n","                    #attention_mask=~mask\n","                    ).last_hidden_state\n","        #x = self.transformer(x)\n","        x_mask = torch.ones(x.shape, dtype=torch.bool).to(self.device)\n","        pool_tokens = self.pooling(x=x, x_mask=x_mask)\n","        logits = self.to_logits(pool_tokens)\n","        return logits\n","\n","\n","class FTT_plus(TorchUniversalModel):\n","    \"\"\"Mixed data model.\n","\n","    Class for preparing input for DL model with mixed data.\n","\n","    Args:\n","            n_out: Number of output dimensions.\n","            cont_params: Dict with numeric model params.\n","            cat_params: Dict with category model para\n","            **kwargs: Loss, task and other parameters.\n","\n","        \"\"\"\n","\n","    def __init__(\n","            self,\n","            n_out: int = 1,\n","            cont_params = None,\n","            cat_params = None,\n","            **kwargs,\n","    ):\n","        # init parent class (need some helper functions to be used)\n","        super(FTT_plus, self).__init__(**{\n","                **kwargs,\n","                \"cont_params\": cont_params,\n","                \"cat_params\": cat_params,\n","                \"torch_model\": None, # dont need any model inside parent class\n","        })\n","        \n","        n_in = 0\n","        # add cont columns processing\n","        self.cont_embedder = PLREmbedding(**cont_params)\n","        n_in += self.cont_embedder.get_out_shape()\n","        \n","        # add cat columns processing\n","        self.cat_embedder = WeightedCatEmbedding(**cat_params)\n","        n_in += self.cat_embedder.get_out_shape()\n","        \n","        self.torch_model = FTTransformer2(\n","                **{\n","                    **kwargs,\n","                    **{\"n_in\": n_in, \"n_out\": n_out},\n","                }\n","        )\n","    \n","    def get_logits(self, inp) -> torch.Tensor:\n","        outputs = []\n","        outputs.append(self.cont_embedder(inp))\n","        outputs.append(self.cat_embedder(inp))\n","        \n","        if len(outputs) > 1:\n","            output = torch.cat(outputs, dim=1)\n","        else:\n","            output = outputs[0]\n","        \n","        logits = self.torch_model(output)\n","        return logits\n","\n","def myround(x, base=1000):\n","    return base * np.round(np.float32(x) / base)\n","        \n","cb_params = {\n","        \"task_type\": \"GPU\",\n","        #\"thread_count\": 4,\n","        \"random_seed\": 42,\n","        #\"learning_rate\": 0.03,\n","        #\"l2_leaf_reg\": 1e-2,\n","        \"bootstrap_type\": 'Bernoulli', #\"Bernoulli\",\n","        # \"bagging_temperature\": 1,\n","        'subsample': 0.5,\n","        \"grow_policy\": \"SymmetricTree\",\n","        \"max_depth\": 9,\n","        #\"min_data_in_leaf\": 50000,\n","        \"one_hot_max_size\": 2, #10,\n","        \"fold_permutation_block\": 5,\n","        \"boosting_type\": \"Ordered\",\n","        \"boost_from_average\": False,#True,\n","        \"od_type\": \"Iter\",\n","        \"od_wait\": 200,\n","        \"max_bin\": 32, #32,\n","        \"feature_border_type\": \"GreedyLogSum\",\n","        \"nan_mode\": \"Min\",\n","        # \"silent\": False,\n","        \"verbose\": 100,\n","        \"allow_writing_files\": False,\n","        #'objective': 'Tweedie:variance_power=1.5',\n","    \n","    'num_trees':50000,\n","    #'learning_rate': 0.02,\n","    #'random_strength':0,\n","    'l2_leaf_reg': 5.5,\n","    #'max_depth': 9\n","    \n","    }\n","\n","lgb_params = {\n","        \"objective\": \"rmse\",  \n","        'metric': 'rmse', 'num_trees': 50000, \n","        'max_depth': 7,\n","        'min_data_in_leaf': 1000,\n","        'bagging_fraction': 0.5,\n","        'reg_alpha': 0, \n","        'reg_lambda': 2,\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.metrics import log_loss\n","import joblib\n","import os \n","res = {}\n","\n","for clip_name, clip in zip(['woclip', 'clip'], [999_999_999, 500_000]):\n","    for data_name, add_data in zip(['data', 'wodata'], [train2_add, None]):\n","        print(clip_name, data_name)\n","        # denselight classification\n","        def ll(x, y):\n","            return log_loss(x, y, labels=np.arange(y.shape[1]))\n","        task = Task('multiclass', metric=ll)\n","        oof = np.zeros(len(train2))\n","        pred = 0\n","        pred_add = 0\n","        models = {}\n","        if not os.path.isfile(f'denselight_cls_{clip_name}_{data_name}.jbl'):\n","            for fold, (tr_idx, te_idx) in enumerate(KFold(n_splits=10, random_state=42, shuffle=True).split(train2['price'], train2['price'], None)):\n","                x_tr, x_vl = train2.iloc[tr_idx].reset_index(drop=True), train2.iloc[te_idx].reset_index(drop=True)\n","                if add_data is not None:\n","                    x_tr = pd.concat([x_tr, train2_add], axis=0).reset_index(drop=True)\n","                \n","                x_tr['price'] = np.clip(x_tr['price'], 0, clip)\n","\n","                x_tr['price'] = x_tr['price'].apply(myround )\n","                x_vl['price'] = x_vl['price'].apply(myround )\n","\n","                sorted_labels = np.array(sorted(np.unique(x_tr['price'])))\n","                sorted_labels2 = np.array(sorted(np.unique(x_vl['price'])))\n","\n","                mapping = {}\n","                for curr in sorted_labels2:\n","                    diff = np.abs(sorted_labels - curr)\n","                    idx = np.argmin(diff)\n","                    mapping[curr] = sorted_labels[idx]\n","                x_vl['price'] = x_vl['price'].map(mapping)\n","                automl1 = TabularAutoML(\n","                task = task, \n","                timeout = 600 * 3600,\n","                cpu_limit = 2, \n","                gpu_ids = '0',\n","                selection_params = {'mode': 0},\n","                general_params = {\"use_algos\": [['denselight']]}, \n","                nn_params = {\n","                    \"n_epochs\": 20, \n","                    \"bs\": 1024//2, \n","                    \"num_workers\": 0, \n","                    \"path_to_save\": None, \n","                    \"freeze_defaults\": True,\n","                    \"cont_embedder\": 'plr',\n","                    'cat_embedder': 'weighted',\n","                    'act_fun': 'GELU',#'SiLU',\n","                    \"hidden_size\": [64, 32], #32,\n","                    'stop_by_metric': True,\n","                    'embedding_size': 37,#32,\n","                    'verbose_bar': True,\n","                    'input_bn': False,\n","                    'opt_params': { 'lr': 0.0003 , 'weight_decay': 0 }\n","                },\n","                nn_pipeline_params = {\"use_qnt\": True, \"use_te\": False},\n","                reader_params = {'n_jobs': 12, 'cv': 10, 'random_state': 42, 'advanced_roles': False})\n","\n","                out_of_fold_predictions = automl1.fit_predict(\n","                    x_tr, valid_data = x_vl, \n","                    roles = {\n","                        'target': 'price',\n","                        'drop': ['id'],\n","                        \n","                    }, \n","                    verbose = 1)\n","                values = np.array(list(automl1.reader.class_mapping.keys()))\n","\n","                oof[te_idx] = (out_of_fold_predictions.data @ values.reshape(-1, 1)).flatten()\n","                pred += (automl1.predict(test2).data @ values.reshape(-1, 1)).flatten() / 10\n","                pred_add += (automl1.predict(test2_add).data  @ values.reshape(-1, 1)).flatten() / 10\n","\n","                models[fold] = automl1\n","            res_dl_cls = {'oof': oof, 'pred': pred, 'pred_add': pred_add, 'models': models}\n","            joblib.dump(res_dl_cls, f'denselight_cls_{clip_name}_{data_name}.jbl')\n","            print(f'denselight_cls_{clip_name}_{data_name}', np.sqrt(mean_squared_error(train2['price'], oof)))\n","\n","        # ftt classification\n","        task = Task('multiclass', metric=ll)\n","        oof = np.zeros(len(train2))\n","        pred = 0\n","        pred_add = 0\n","        models = {}\n","        if not os.path.isfile(f'ftt_cls_{clip_name}_{data_name}.jbl'):\n","            for fold, (tr_idx, te_idx) in enumerate(KFold(n_splits=10, random_state=42, shuffle=True).split(train2['price'], train2['price'], None)):\n","                x_tr, x_vl = train2.iloc[tr_idx].reset_index(drop=True), train2.iloc[te_idx].reset_index(drop=True)\n","                if add_data is not None:\n","                    x_tr = pd.concat([x_tr, train2_add], axis=0).reset_index(drop=True)\n","                \n","                x_tr['price'] = np.clip(x_tr['price'], 0, clip)\n","\n","                x_tr['price'] = x_tr['price'].apply(myround )\n","                x_vl['price'] = x_vl['price'].apply(myround )\n","\n","                sorted_labels = np.array(sorted(np.unique(x_tr['price'])))\n","                sorted_labels2 = np.array(sorted(np.unique(x_vl['price'])))\n","\n","                mapping = {}\n","                for curr in sorted_labels2:\n","                    diff = np.abs(sorted_labels - curr)\n","                    idx = np.argmin(diff)\n","                    mapping[curr] = sorted_labels[idx]\n","                x_vl['price'] = x_vl['price'].map(mapping)\n","\n","                automl1 = TabularAutoML(debug=True,\n","                task = task, \n","                timeout = 600 * 3600,\n","                cpu_limit = 2, \n","                gpu_ids = '0',\n","                selection_params = {'mode': 0},\n","                general_params = {\"use_algos\": [[FTT_plus]]}, \n","                nn_params = {\n","                    \"n_epochs\": 20, \n","                    \"bs\": 1024//2, \n","                    \"num_workers\": 0, \n","                    \"path_to_save\": None, \n","                    \"freeze_defaults\": True,\n","                    \"cont_embedder\": 'plr',\n","                    'cat_embedder': 'weighted',\n","                    'act_fun': 'GELU',#'SiLU',\n","                    \"hidden_size\": [64, 32], #32,\n","                    'stop_by_metric': True,\n","                    'embedding_size': 128,#32,\n","                    'verbose_bar': True,\n","                    'input_bn': False,\n","                    'opt_params': { 'lr': 0.0003 , 'weight_decay': 0 }, \"model_with_emb\": True,\n","                },\n","                nn_pipeline_params = {\"use_qnt\": True, \"use_te\": False},\n","                reader_params = {'n_jobs': 12, 'cv': 10, 'random_state': 42, 'advanced_roles': False})\n","\n","                out_of_fold_predictions = automl1.fit_predict(\n","                    x_tr, valid_data = x_vl, \n","                    roles = {\n","                        'target': 'price',\n","                        'drop': ['id'],\n","                        \n","                    }, \n","                    verbose = 1)\n","                    \n","                values = np.array(list(automl1.reader.class_mapping.keys()))\n","\n","                oof[te_idx] = (out_of_fold_predictions.data @ values.reshape(-1, 1)).flatten()\n","                pred += (automl1.predict(test2).data @ values.reshape(-1, 1)).flatten() / 10\n","                pred_add += (automl1.predict(test2_add).data  @ values.reshape(-1, 1)).flatten() / 10\n","\n","                models[fold] = automl1\n","            res_dl_cls = {'oof': oof, 'pred': pred, 'pred_add': pred_add, 'models': models}\n","            joblib.dump(res_dl_cls, f'ftt_cls_{clip_name}_{data_name}.jbl')\n","            print(f'ftt_cls_{clip_name}_{data_name}', np.sqrt(mean_squared_error(train2['price'], oof)))\n","\n","        # denselight regression\n","        task = Task('reg')\n","        oof = np.zeros(len(train2))\n","        pred = 0\n","        pred_add = 0\n","        models = {}\n","        if not os.path.isfile(f'denselight_reg_{clip_name}_{data_name}.jbl'):\n","            for fold, (tr_idx, te_idx) in enumerate(KFold(n_splits=10, random_state=42, shuffle=True).split(train2['price'], train2['price'], None)):\n","                x_tr, x_vl = train2.iloc[tr_idx].reset_index(drop=True), train2.iloc[te_idx].reset_index(drop=True)\n","                if add_data is not None:\n","                    x_tr = pd.concat([x_tr, train2_add], axis=0).reset_index(drop=True)\n","                \n","                x_tr['price'] = np.clip(x_tr['price'], 0, clip)\n","\n","                scaler = RobustScaler()\n","                scaler.fit(x_tr['price'].values.reshape(-1, 1))\n","                x_tr['price'] = scaler.transform(x_tr['price'].values.reshape(-1, 1))\n","                x_vl['price'] = scaler.transform(x_vl['price'].values.reshape(-1, 1))\n","\n","                automl1 = TabularAutoML(debug=True,\n","                task = task, \n","                timeout = 600 * 3600,\n","                cpu_limit = 2, \n","                gpu_ids = '0',\n","                selection_params = {'mode': 0},\n","                general_params = {\"use_algos\": [['denselight']]}, \n","                nn_params = {\n","                    \"n_epochs\": 10, \n","                    \"bs\": 1024//2, \n","                    \"num_workers\": 0, \n","                    \"path_to_save\": None, \n","                    \"freeze_defaults\": True,\n","                    \"cont_embedder\": 'plr',\n","                    'cat_embedder': 'weighted',\n","                    'act_fun': 'GELU',#'SiLU',\n","                    \"hidden_size\": [64, 32], #32,\n","                    'stop_by_metric': True,\n","                    'embedding_size': 37,#32,\n","                    'verbose_bar': True,\n","                    'input_bn': False,\n","                    'opt_params': { 'lr': 0.0003 , 'weight_decay': 0 }\n","                },\n","                nn_pipeline_params = {\"use_qnt\": True, \"use_te\": False},\n","                reader_params = {'n_jobs': 12, 'cv': 10, 'random_state': 42, 'advanced_roles': False})\n","\n","                out_of_fold_predictions = automl1.fit_predict(\n","                    x_tr, valid_data = x_vl, \n","                    roles = {\n","                        'target': 'price',\n","                        'drop': ['id'],\n","                        \n","                    }, \n","                    verbose = 1)\n","                oof[te_idx] = scaler.inverse_transform(out_of_fold_predictions.data.flatten().reshape(-1, 1)).flatten()\n","                pred += scaler.inverse_transform(automl1.predict(test2).data.flatten().reshape(-1, 1)).flatten() / 10\n","                pred_add += scaler.inverse_transform(automl1.predict(test2_add).data.flatten().reshape(-1, 1)).flatten() / 10\n","\n","                models[fold] = automl1\n","            res_dl_r = {'oof': oof, 'pred': pred, 'pred_add': pred_add, 'models': models}\n","            joblib.dump(res_dl_r, f'denselight_reg_{clip_name}_{data_name}.jbl')\n","            print(f'denselight_reg_{clip_name}_{data_name}', np.sqrt(mean_squared_error(train2['price'], oof)))\n","\n","        # ftt regression\n","        task = Task('reg')\n","        oof = np.zeros(len(train2))\n","        pred = 0\n","        pred_add = 0\n","        models = {}\n","        if not os.path.isfile(f'ftt_reg_{clip_name}_{data_name}.jbl'):\n","            for fold, (tr_idx, te_idx) in enumerate(KFold(n_splits=10, random_state=42, shuffle=True).split(train2['price'], train2['price'], None)):\n","                x_tr, x_vl = train2.iloc[tr_idx].reset_index(drop=True), train2.iloc[te_idx].reset_index(drop=True)\n","                if add_data is not None:\n","                    x_tr = pd.concat([x_tr, train2_add], axis=0).reset_index(drop=True)\n","                \n","                x_tr['price'] = np.clip(x_tr['price'], 0, clip)\n","\n","                scaler = RobustScaler()\n","                scaler.fit(x_tr['price'].values.reshape(-1, 1))\n","                x_tr['price'] = scaler.transform(x_tr['price'].values.reshape(-1, 1))\n","                x_vl['price'] = scaler.transform(x_vl['price'].values.reshape(-1, 1))\n","\n","                automl1 = TabularAutoML(debug=True,\n","                task = task, \n","                timeout = 600 * 3600,\n","                cpu_limit = 2, \n","                gpu_ids = '0',\n","                selection_params = {'mode': 0},\n","                general_params = {\"use_algos\": [[FTT_plus]]}, \n","                nn_params = {\n","                    \"n_epochs\": 10, \n","                    \"bs\": 1024//2, \n","                    \"num_workers\": 0, \n","                    \"path_to_save\": None, \n","                    \"freeze_defaults\": True,\n","                    \"cont_embedder\": 'plr',\n","                    'cat_embedder': 'weighted',\n","                    'act_fun': 'GELU',#'SiLU',\n","                    \"hidden_size\": [64, 32], #32,\n","                    'stop_by_metric': True,\n","                    'embedding_size': 128,#128 - tiny, 512 - small ,#32,\n","                    'verbose_bar': True,\n","                    #'init_bias': False,\n","                    'input_bn': False,\n","                    #'sch': 'CosineAnnealingLR',\n","                    #'scheduler_params': { 'T_max': 10, 'eta_min':0, 'last_epoch':-1, 'verbose':None},\n","                    #\"snap_params\": { 'k': 3, 'early_stopping': True, 'patience': 3, 'swa': False }, \n","                    'opt_params': { 'lr': 0.0003 , 'weight_decay': 0 },\n","                    \"model_with_emb\": True,\n","                },nn_pipeline_params = {\"use_qnt\": True, \"use_te\": False},\n","                reader_params = {'n_jobs': 12, 'cv': 10, 'random_state': 42, 'advanced_roles': False})\n","                \n","\n","                out_of_fold_predictions = automl1.fit_predict(\n","                    x_tr, valid_data = x_vl, \n","                    roles = {\n","                        'target': 'price',\n","                        'drop': ['id'],\n","                        \n","                    }, \n","                    verbose = 1)\n","                oof[te_idx] = scaler.inverse_transform(out_of_fold_predictions.data.flatten().reshape(-1, 1)).flatten()\n","                pred += scaler.inverse_transform(automl1.predict(test2).data.flatten().reshape(-1, 1)).flatten() / 10\n","                pred_add += scaler.inverse_transform(automl1.predict(test2_add).data.flatten().reshape(-1, 1)).flatten() / 10\n","\n","                models[fold] = automl1\n","            res_dl_r = {'oof': oof, 'pred': pred, 'pred_add': pred_add, 'models': models}\n","            joblib.dump(res_dl_r, f'ftt_reg_{clip_name}_{data_name}.jbl')\n","            print(f'ftt_reg_{clip_name}_{data_name}', np.sqrt(mean_squared_error(train2['price'], oof)))\n","\n","        # catboost regression\n","        task = Task('reg')\n","        oof = np.zeros(len(train2))\n","        pred = 0\n","        pred_add = 0\n","        models = {}\n","        if not os.path.isfile(f'catboost_{clip_name}_{data_name}.jbl'):\n","            for fold, (tr_idx, te_idx) in enumerate(KFold(n_splits=10, random_state=42, shuffle=True).split(train2['price'], train2['price'], None)):\n","                x_tr, x_vl = train2.iloc[tr_idx].reset_index(drop=True), train2.iloc[te_idx].reset_index(drop=True)\n","                if add_data is not None:\n","                    x_tr = pd.concat([x_tr, train2_add], axis=0).reset_index(drop=True)\n","                \n","                x_tr['price'] = np.clip(x_tr['price'], 0, clip)\n","\n","                automl1 = TabularAutoML(debug=True,\n","                task = task, \n","                timeout = 600 * 3600,\n","                cpu_limit = 2,\n","                gpu_ids = '0',\n","                selection_params = {'mode': 0},\n","                general_params = {\"use_algos\": [['cb']]},\n","                cb_params = {'default_params': cb_params,\n","                            'freeze_defaults': True},\n","                reader_params = {'n_jobs': 12, 'cv': 10, 'random_state': 42, 'advanced_roles': True})\n","\n","                out_of_fold_predictions = automl1.fit_predict(\n","                    x_tr, valid_data = x_vl, \n","                    roles = {\n","                        'target': 'price',\n","                        'drop': ['id'],\n","                        \n","                    }, \n","                    verbose = 1)\n","                oof[te_idx] = out_of_fold_predictions.data.flatten()\n","                pred += automl1.predict(test2).data.flatten() / 10\n","                pred_add += automl1.predict(test2_add).data.flatten() / 10\n","\n","                models[fold] = automl1\n","            res_catboost = {'oof': oof, 'pred': pred, 'pred_add': pred_add, 'models': models}\n","            joblib.dump(res_catboost, f'catboost_{clip_name}_{data_name}.jbl')\n","\n","            print(f'catboost_{clip_name}_{data_name}', np.sqrt(mean_squared_error(train2['price'], oof)))\n","\n","\n","        # lgb regression\n","        task = Task('reg')\n","        oof = np.zeros(len(train2))\n","        pred = 0\n","        pred_add = 0\n","        models = {}\n","        if not os.path.isfile(f'lgb_{clip_name}_{data_name}.jbl'):\n","            for fold, (tr_idx, te_idx) in enumerate(KFold(n_splits=10, random_state=42, shuffle=True).split(train2['price'], train2['price'], None)):\n","                x_tr, x_vl = train2.iloc[tr_idx].reset_index(drop=True), train2.iloc[te_idx].reset_index(drop=True)\n","                if add_data is not None:\n","                    x_tr = pd.concat([x_tr, train2_add], axis=0).reset_index(drop=True)\n","                \n","                x_tr['price'] = np.clip(x_tr['price'], 0, clip)\n","\n","                automl1 = TabularAutoML(debug=True,\n","                task = task, \n","                timeout = 600 * 3600,\n","                cpu_limit = 2,\n","                gpu_ids = '0',\n","                selection_params = {'mode': 0},\n","                general_params = {\"use_algos\": [['lgb']]},\n","                lgb_params = {'default_params': lgb_params,\n","                            'freeze_defaults': True},\n","                reader_params = {'n_jobs': 12, 'cv': 10, 'random_state': 42, 'advanced_roles': True})\n","\n","                out_of_fold_predictions = automl1.fit_predict(\n","                    x_tr, valid_data = x_vl, \n","                    roles = {\n","                        'target': 'price',\n","                        'drop': ['id'],\n","                        \n","                    }, \n","                    verbose = 1)\n","                oof[te_idx] = out_of_fold_predictions.data.flatten()\n","                pred += automl1.predict(test2).data.flatten() / 10\n","                pred_add += automl1.predict(test2_add).data.flatten() / 10\n","\n","                models[fold] = automl1\n","            res_lgb = {'oof': oof, 'pred': pred, 'pred_add': pred_add, 'models': models}\n","            joblib.dump(res_lgb, f'lgb_{clip_name}_{data_name}.jbl')\n","            print(f'lgb_{clip_name}_{data_name}', np.sqrt(mean_squared_error(train2['price'], oof)))\n","\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8930475,"sourceId":73291,"sourceType":"competition"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"icecube_jup","language":"python","name":"icecube"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
